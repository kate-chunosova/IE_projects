{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML3_NN_XOR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC2ozo1VnePr",
        "colab_type": "text"
      },
      "source": [
        "# A Multilayer Neural Network implementation for solving the XOR problem using Tensorflow.\n",
        "\n",
        "This code could be significantly simplified by using Keras or other libraries, but the aim here is to understand the step-by-step process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "271QUZXLnfv8",
        "colab_type": "code",
        "outputId": "8eb6a80f-3606-43ee-96d0-99f37901d7ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGJ3ntvI0IU",
        "colab_type": "text"
      },
      "source": [
        "Defining the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yM1IJSV0whAd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + math.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSV6gTvtJjhk",
        "colab_type": "text"
      },
      "source": [
        "Defining the XOR problem input space. Note that even though we use a binary input the output of the neural net will be a real number.\n",
        "\n",
        "X is a bidimensional input (x1, x2) and Y are the corresponding labels following the XOR output function.\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1q0y13JLtQqGTL_J4PWz9q5Hr8gvlcrzt)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ung_P3yXnhil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# XOR definition\n",
        "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
        "Y = [[0], [1], [1], [0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrp_oT47nizR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Network Parameters\n",
        "N_STEPS = 100000\n",
        "N_TRAINING = len(X) # in this case, 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWl5fCDtL1-6",
        "colab_type": "text"
      },
      "source": [
        "Defining the ANN topology. In this case  2 - 2 - 1\n",
        "\n",
        "![texto alternativo](https://drive.google.com/uc?id=1JOgtE0qjqvDTMFE5drMXAeZGV33Yrtv0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RjGNvaInkFg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N_INPUT_NODES = 2\n",
        "N_HIDDEN_NODES = 2\n",
        "N_OUTPUT_NODES = 1\n",
        "LEARNING_RATE = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSrQUKSXNNg1",
        "colab_type": "text"
      },
      "source": [
        "Defining the placeholders and variables. The Variable are the params to train, placeholders are the 'room' for the inputs & labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ivkp9S03nlpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create placeholders for variables and define Neural Network structure\n",
        "x_ = tf.placeholder(tf.float32, shape=[N_TRAINING, N_INPUT_NODES], name=\"x-input\")\n",
        "y_ = tf.placeholder(tf.float32, shape=[N_TRAINING, N_OUTPUT_NODES], name=\"y-input\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOIDt2EnnBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "theta1 = tf.Variable(tf.random_uniform([N_INPUT_NODES, N_HIDDEN_NODES], -1, 1), name=\"theta1\")\n",
        "theta2 = tf.Variable(tf.random_uniform([N_HIDDEN_NODES, N_OUTPUT_NODES], -1, 1), name=\"theta2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHIgEwudnolU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias1 = tf.Variable(tf.zeros([N_HIDDEN_NODES]), name=\"bias1\")\n",
        "bias2 = tf.Variable(tf.zeros([N_OUTPUT_NODES]), name=\"bias2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My8-yg3tNZDF",
        "colab_type": "text"
      },
      "source": [
        "Defining the function of every layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtVP6eaqnqU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use a sigmoidal activation function\n",
        "layer1 = tf.sigmoid(tf.matmul(x_, theta1) + bias1)\n",
        "output = tf.sigmoid(tf.matmul(layer1, theta2) + bias2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8las2XGwOChN",
        "colab_type": "text"
      },
      "source": [
        "Defining the cost function. In this case Cross entropy.\n",
        "\n",
        "$C=−1/n∑x[yln(o)+(1−y)ln(1−o)]$\n",
        "\n",
        "where $o$ is the ouput of the net.\n",
        "\n",
        "The cross entropy cost function is basically a suitable function to measure our errors (This is what you need to know!)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT-NFyELnrt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cost = - tf.reduce_mean((y_ * tf.log(output)) + (1 - y_) * tf.log(1.0 - output))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6N3ZI_KPx0X",
        "colab_type": "text"
      },
      "source": [
        "Defining the Learning Processor --> Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB873pAUPvsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_step = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4p_mTbSntC8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess = tf.Session()\n",
        "sess.run(init)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnHKv3KgQBMp",
        "colab_type": "text"
      },
      "source": [
        "Training and testing our algorithm. Note that the whole space is just four inputs, so there are not testing or training datasets. It is the same dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEdShnM3nuoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(100000):\n",
        "  sess.run(train_step, feed_dict={x_: X, y_: Y})\n",
        "  if i % 10000 == 0:\n",
        "      print('Batch ', i)\n",
        "      print('Input:', X, 'Output(Inference)', sess.run(tf.transpose(output), feed_dict={x_: X, y_: Y}))\n",
        "      print('Cost ', sess.run(cost, feed_dict={x_: X, y_: Y}))\n",
        "      print(\"theta1:\", sess.run(theta1))\n",
        "      print(\"theta2:\", sess.run(theta2))\n",
        "      print(\"bias1:\", sess.run(bias1))\n",
        "      print(\"bias2:\", sess.run(bias2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyG4ZZHwXY2A",
        "colab_type": "text"
      },
      "source": [
        "Printing the final trained params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEIfBxYBQkPd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(sess.run(theta1))\n",
        "print(sess.run(theta2))\n",
        "print(sess.run(bias1))\n",
        "print(sess.run(bias2))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}